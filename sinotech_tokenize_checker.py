# -*- coding: utf-8 -*-
"""sinotech_professionalism_checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/javatta/35de4c3217f210f52868c1acb96087fe/sinotech_professionalism_checker.ipynb
"""

# Установка зависимостей
import os
import requests
import re
import time
import pandas as pd
import spacy
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import Counter
from nltk import ngrams
from spacy.lang.zh.stop_words import STOP_WORDS

# Загрузка китайской модели
try:
    nlp = spacy.load("zh_core_web_sm")
except OSError:
    !python -m spacy download zh_core_web_sm
    nlp = spacy.load("zh_core_web_sm")

# Скачиваем шрифт для графиков
!wget -q "https://github.com/google/fonts/raw/main/ofl/notosanssc/NotoSansSC%5Bwght%5D.ttf" -O font.ttf
prop = fm.FontProperties(fname='font.ttf')

# Список проектов и прямые ссылки на их RAW README
projects = {
    "HanLP": "https://raw.githubusercontent.com/hankcs/HanLP/master/README.md",
    "LTP": "https://raw.githubusercontent.com/HIT-SCIR/ltp/master/README.md",
    "THULAC": "https://raw.githubusercontent.com/thunlp/THULAC-Python/master/README.md",
    "bert4torch": "https://raw.githubusercontent.com/Tongjilibo/bert4torch/master/README.md",
    "Chinese-CLIP": "https://raw.githubusercontent.com/OFA-Sys/Chinese-CLIP/master/README.md",
    "jieba": "https://raw.githubusercontent.com/fxsjy/jieba/master/README.md",
    "FastHan": "https://raw.githubusercontent.com/fastnlp/fastHan/master/README.md"
}

def download_readmes(project_dict, folder="raw_data"):
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"Папка '{folder}' создана")

    for name, url in project_dict.items():
        try:
            response = requests.get(url)
            response.raise_for_status() # Проверка на ошибки

            # Сохраняем файл
            filename = os.path.join(folder, f"{name}.md")
            with open(filename, "w", encoding="utf-8") as f:
                f.write(response.text)
            print(f"Скачан: {name}")
        except Exception as e:
            print(f"Ошибка при скачивании {name}: {e}")

def build_dataset(folder="raw_data"):
    data = []
    for filename in os.listdir(folder):
        if filename.endswith(".md") or filename.endswith(".txt"):
            path = os.path.join(folder, filename)
            print(path)
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
                data.append({
                    "name": filename,
                    "source": "README",
                    "text": content
                })

    df = pd.DataFrame(data)
    return df

download_readmes(projects)

df = build_dataset()

print("\n Итоговая таблица:")
print(df.head())

def clean_markdown(text):
    # Удаляем блоки кода
    text = re.sub(r'```[\s\S]*?```', '', text)

    # Удаляем теги
    text = re.sub(r'<[^>]+>', '', text)

    # Удаляем короткий код внутри строк
    text = re.sub(r'`.*?`', '', text)

    # Удаляем изображения и ссылки
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)

    # Удаляем обычные utl
    text = re.sub(r'http\S+', '', text)

    # Cпецсимволы
    text = re.sub(r'[#*_\->|+=]', ' ', text)

    # Cлишком много пробелов и переносов строк
    text = re.sub(r'\s+', ' ', text).strip()

    # удаляет не  иероглиф, не буква и не пробел
    text = re.sub(r'[^\u4e00-\u9fa5\s]', ' ', text)

    text = re.sub(r'\s+', ' ', text).strip()

    return text

df['cleaned_text'] = df['text'].apply(clean_markdown)

print("Оригинал:")
print(df['text'].iloc[0][:200])

print("\nЧистый текст")
print(df['cleaned_text'].iloc[0][:200])

df

# Китайская модель
nlp = spacy.load("zh_core_web_sm")

def process_chinese_text(text):
    doc = nlp(text[:100000])

    # Собираем список кортежей (слово, часть речи) и фильтруем
    tokens_data = [
        (token.text, token.pos_)
        for token in doc
        if not token.is_punct and not token.is_space and token.pos_ != "NUM"
    ]
    return tokens_data

df['tokens_pos'] = df['cleaned_text'].apply(process_chinese_text)

# Разворачиваем все слова в один большой список
all_tokens = [item for sublist in df['tokens_pos'] for item in sublist]

print(f"Всего слов обработано: {len(all_tokens)}")

df

# Считаем частоту слов
word_freq = Counter([word for word, pos in all_tokens])

print("Топ-20 самых частых слов: ")
for word, count in word_freq.most_common(20):
    print(f"{word}: {count}")

#ТОКЕНИЗИРУЕМ

def is_chinese(text):
    # Проверяет, есть ли китайские иероглифы
    return bool(re.search(r'[\u4e00-\u9fa5]', text))

def process_only_chinese(text):
    # nlp — это загруженная ранее zh_core_web_sm
    doc = nlp(text[:100000])
    filtered_tokens = []

    for token in doc:
        word = token.text
        if (
            is_chinese(word)                # Только китайские иероглифы
            and word not in STOP_WORDS       # Нет в стоп-листе
            and len(word) > 1                # Больше 1 иероглифа
            and not token.is_space           # Не пробел
            and token.pos_ not in ["PUNCT", "NUM"] # Не знаки и не цифры
        ):
            filtered_tokens.append((word,token.pos_))
    return filtered_tokens

df['tokens_pos'] = df['cleaned_text'].apply(process_only_chinese)

all_tokens = [item for sublist in df['tokens_pos'] for item in sublist]

# Считаем частоту слов
word_freq = Counter([word for word in all_tokens])

print("Только китайский топ 20-униграмм:")
for word, count in word_freq.most_common(20):
    print(f"{word}: {count}")
df

words_only = [word for word, pos in all_tokens]

# Создаем пары соседних слов
bigrams = list(ngrams(words_only, 2))

bigram_freq = Counter(bigrams)

print("\n Топ-20 биграмм:")
for gram, count in bigram_freq.most_common(20):
    print(f"{''.join(gram)}: {count}")

# Ссылка на дерево файлов проекта
api_url = "https://api.github.com/repos/PaddlePaddle/PaddleNLP/git/trees/develop?recursive=1"

response = requests.get(api_url)
data = response.json()

# Проверяем, что ответ не пустой
if 'tree' in data:
    all_files = data['tree']
    print(f"Всего файлов в проекте: {len(all_files)}")

md_files = []

for item in all_files:
    path = item['path']

    # Проверяем тип = файл
    if item['type'] == 'blob' and path.endswith('.md'):

        if 'docs/' in path:
          if ('zh' in path or 'ch' in path) and 'en' not in path:
            md_files.append(path)

print(f"Найдено китайских файлов: {len(md_files)}")
print(md_files[:10])

all_texts = []
# Число предыдущего подсчета файлов
for path in md_files:
    # Сформируем ссылку
    raw_url = f'https://raw.githubusercontent.com/PaddlePaddle/PaddleNLP/develop/{path}'

    response = requests.get(raw_url)

    all_texts.append({
        "name": path,
        'source': 'API',
        "text": response.text})

    time.sleep(0.1)
df_paddle = pd.DataFrame(all_texts)
df_paddle

# Применяем очистку к df_paddle
df_paddle['cleaned_text'] = df_paddle['text'].apply(clean_markdown)
df_paddle[:3]
df_paddle.tail(5)
# Применяем функцию к очищенному тексту
df_paddle['tokens_pos'] = df_paddle['cleaned_text'].apply(process_only_chinese)

# Сплющиваем колонку tokens_pos в один длинный список
all_tokens_paddle = [item for sublist in df_paddle['tokens_pos'] for item in sublist]
# Фильр пустого содержания
df_paddle_clean = df_paddle[df_paddle['tokens_pos'].apply(len) > 0]
final_df = pd.concat([df, df_paddle_clean], ignore_index=True)
final_df

# Считаем частоту
from collections import Counter
word_freq_paddle = Counter([word for word, pos in all_tokens_paddle])

print("Топ-20 из PaddleNLP: ")
for word, count in word_freq_paddle.most_common(20):
    print(f"{word}: {count}")

# Продолжаем считать топы
verbs_only = [word for sublist in final_df['tokens_pos'] for word, pos in sublist if pos == "VERB"]
verb_freq = Counter(verbs_only)
print("Топ-20 ГЛАГОЛОВ:")
for word, count in verb_freq.most_common(20):
    print(f"{word}: {count}")

nouns_only = [word for sublist in final_df['tokens_pos'] for word, pos in sublist if pos == "NOUN"]
verb_freq = Counter(nouns_only)
print("\nТоп-20 СУЩЕСТВИТЕЛЬНЫХ:")
for word, count in verb_freq.most_common(20):
    print(f"{word}: {count}")



# Добавляем еще один источник в API
# Получаем дерево файлов проекта HanLP
# Используем ветку master
url = "https://api.github.com/repos/hankcs/HanLP/git/trees/master?recursive=1"
res = requests.get(url)
repo_data = res.json()

hanlp_paths = []

if 'tree' in repo_data:
    for item in repo_data['tree']:
        path = item['path']
        # Условие: это файл (.blob), расширение .md, лежит в docs/
        if item['type'] == 'blob' and path.endswith('.md') and path.startswith('docs/'):
            # Исключаем README, так как он обычно дублирует информацию
            if 'README' not in path:
                hanlp_paths.append(path)

print(f"Файлов документации в HanLP: {len(hanlp_paths)}")



# Добавляем новый API (HanLP) в датафрейм
new_docs = []

for path in hanlp_paths:
    raw_url = f"https://raw.githubusercontent.com/hankcs/HanLP/master/{path}"

    try:
        response = requests.get(raw_url)
        if response.status_code == 200:
            text_content = response.text
            cleaned = clean_markdown(text_content)
            tokens = process_only_chinese(cleaned)

            if len(tokens) > 0:
                new_docs.append({
                    "name": f"hanlp/{path}",
                    "source": "API",
                    "text": text_content,
                    "cleaned_text": cleaned,
                    "tokens_pos": tokens
                })
                print(f"Загружен: {path}")
    except Exception as e:
        print(f"Ошибка{path}: {e}")

    time.sleep(0.1)

# Создаем DataFrame из новых данных
df_hanlp = pd.DataFrame(new_docs)
final_df = pd.concat([final_df, df_hanlp], ignore_index=True)
print(f"\nВ базе документов: {len(final_df)}")



# дОБавляем книги в корпус
import os
# Читаем файл
file_path = 'cips2021.txt'

if os.path.exists(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        raw_content = f.read()

    # Создаем временную таблицу для этого файла
    new_data = {
        "name": "My_NLP_Book",
        "source": "Book",
        "text": raw_content
    }
    df_new = pd.DataFrame([new_data])

    df_new['cleaned_text'] = df_new['text'].apply(clean_markdown)

    df_new['tokens_pos'] = df_new['cleaned_text'].apply(process_only_chinese)

    final_df = pd.concat([final_df, df_new], ignore_index=True)

    print(f"Файл {file_path} добавлен.")
    print(f"размер корпуса: {len(final_df)} документов.")
else:
    print(f"Файл {file_path} не найден..")



# дОБавляем книги в корпус
import os
# Читаем файл
file_path = '自然语言.txt'

if os.path.exists(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        raw_content = f.read()

    # Создаем временную таблицу для этого файла
    new_data = {
        "name": "My_NLP_Book_2",
        "source": "Book",
        "text": raw_content
    }
    df_new_1 = pd.DataFrame([new_data])

    df_new_1['cleaned_text'] = df_new_1['text'].apply(clean_markdown)

    df_new_1['tokens_pos'] = df_new_1['cleaned_text'].apply(process_only_chinese)

    final_df = pd.concat([final_df, df_new_1], ignore_index=True)

    print(f"Файл {file_path} добавлен.")
    print(f"размер корпуса: {len(final_df)} документов.")
else:
    print(f"Файл {file_path} не найден..")

clean_nn_pairs = []
# Собираем всё в один список пар
all_global_tokens = [item for sublist in final_df['tokens_pos'] for item in sublist]
for i in range(len(all_global_tokens) - 1):
    word1, pos1 = all_global_tokens[i]
    word2, pos2 = all_global_tokens[i + 1]

    if (
        pos1 == "NOUN" and pos2 == "NOUN"  # Только сущ+сущ
        and word1 != word2                # Убираем повторы
        and len(word1) > 1                # Убираем неполные пары
        and len(word2) > 1
    ):
        clean_nn_pairs.append(f"{word1} {word2}")

from collections import Counter
nn_freq = Counter(clean_nn_pairs)

print("Очищенный топ терминов (Сущ + Сущ):")
for pair, count in nn_freq.most_common(20):
    print(f"{pair}: {count}")
# Проблема: часть речи не всегда понятна даже в связке прим. 自然（сущ)+语言=自然（прил)

clean_vn_pairs = []
for i in range(len(all_global_tokens) - 1):
    word1, pos1 = all_global_tokens[i]
    word2, pos2 = all_global_tokens[i + 1]

    if (
        pos1 == "VERB" and pos2 == "NOUN"
        and word1 != word2
        and len(word1) > 1
        and len(word2) > 1
    ):
        clean_vn_pairs.append(f"{word1} {word2}")
        # Считаем и выводим Топ-20
vn_freq = Counter(clean_vn_pairs)
print("Очищенный топ терминов(Гл + Сущ):")
for pair, count in vn_freq.most_common(20):
    print(f"{pair}: {count}")

prof_collocations = {}

# Берем топ-100 связок для базы
for pair, count in vn_freq.most_common(100):
    parts = pair.split(' ')

    if len(parts) == 2:
        v, n = parts
        if n not in prof_collocations:
            prof_collocations[n] = []
        # Добавляем глагол в список возможных вариантов для этого существительного
        if v not in prof_collocations[n]:
            prof_collocations[n].append(v)

print(f"В базе профессиональных связок объектов: {len(prof_collocations)}")

test_noun = "模型" # Модель
if test_noun in prof_collocations:
    print(f"Для слова '{test_noun}' профи рекомендуют глаголы: {prof_collocations[test_noun]}")

# Собираем все пары гл + сущ из API, README менее формальный, показано в TTR
# Этот кусочек кода написан с ChatGPT
pro_pairs = []
pro_docs = final_df[final_df['source'] == 'API']['tokens_pos']

for sublist in pro_docs:
    for i in range(len(sublist) - 1):
        w1, p1 = sublist[i]
        w2, p2 = sublist[i+1]
        if p1 == "VERB" and p2 == "NOUN" and len(w1) > 1 and len(w2) > 1:
            pro_pairs.append((w1, w2))

# Создаем карту
pro_statistical_map = {}
for v, n in pro_pairs:
    if n not in pro_statistical_map:
        pro_statistical_map[n] = Counter()
    pro_statistical_map[n][v] += 1

print(f"Обработано объектов: {len(pro_statistical_map)}")

def pro_checker(user_text): # Этот кусочек кода написан с ChatGPT
    doc = nlp(user_text)
    user_tokens = [(token.text, token.pos_) for token in doc]

    suggestions = []

    for i in range(len(user_tokens) - 1):
        v_user, v_pos = user_tokens[i]
        n_user, n_pos = user_tokens[i+1]

        if v_pos == "VERB" and n_pos == "NOUN":
            # Проверяем, есть ли это существительное в стандарте
            if n_user in pro_statistical_map:
                # Получаем самые частые глаголы для этого существительного
                top_pro_verbs = [v for v, count in pro_statistical_map[n_user].most_common(3)]

                # Если глагол не в топ-3
                if v_user not in top_pro_verbs:
                    best_option = top_pro_verbs[0]
                    suggestions.append(
                        f"Нетипичное сочетание: '{v_user} {n_user}'.\n"
                        f"В проф. документации с '{n_user}' чаще всего используют: '{best_option}' "
                        f"(всего варианты: {', '.join(top_pro_verbs)})."
                    )
            else:
                # Если существительного нет в базе,ропускаем
                pass

    if not suggestions:
        return ["Стиль соответствует стандарту."]
    return suggestions

#ТЕСТ
test_input = "进行部署模型。 做训练 。"


print("Результаты:")
for res in pro_checker(test_input):
    print(res)

# Создаем графики
source_counts = final_df['source'].value_counts()

names = source_counts.index
values = source_counts.values

plt.figure(figsize=(8, 5))

plt.bar(names, values, color=['#3498db', '#2ecc71'])

plt.ylabel('Количество документов')
plt.xlabel('Источник данных')
plt.title('Объем  IT-корпуса по источникам')
# Красивая подпись сверху
for i, v in enumerate(values):
    plt.text(i, v + 0.01, str(v), ha='center', fontweight='bold', fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

readme_tokens = [
    item for sublist in final_df[final_df['source'] == 'README']['tokens_pos'] # Без sublist too many values
    for item in sublist
]
api_tokens = [
    item for sublist in final_df[final_df['source'] == 'API']['tokens_pos']
    for item in sublist
]
books_tokens = [
    item for sublist in final_df[final_df['source'] == 'Book']['tokens_pos']
    for item in sublist
]
print(f"Слов в куче README: {len(readme_tokens)}")
print(f"Слов в куче API: {len(api_tokens)}")
print(f"Слов в куче Book: {len(books_tokens)}")
def calculate_ttr(tokens_list):

    words = [item[0] for item in tokens_list]

    unique = len(set(words))
    total = len(words)

    return round(unique / total, 3)

# Считаем для каждой кучи отдельно
readme_ttr = calculate_ttr(readme_tokens)
api_ttr = calculate_ttr(api_tokens)
books_ttr = calculate_ttr(books_tokens)
print(f"Глобальное богатство README: {readme_ttr}")
print(f"Глобальное богатство API Docs: {api_ttr}")
print(f"Глобальное богатство Books: {books_ttr}")

labels = ['README', 'API Docs', 'Books']
values = [readme_ttr, api_ttr, books_ttr]

plt.figure(figsize=(8, 5))

plt.bar(labels, values, color=['#3498db', '#2ecc71'])

for i, v in enumerate(values):
    plt.text(i, v + 0.01, str(v), ha='center', fontweight='bold', fontsize=12)

plt.ylim(0, max(values) + 0.1) # Место сверху для красоты
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.ylabel('Индекс TTR')
plt.title('Сравнение лексического разнообразия корпусов')
plt.show()

top_nn = nn_freq.most_common(20)

labels = [item[0] for item in top_nn]
values = [item[1] for item in top_nn]

plt.figure(figsize=(10, 8))

plt.barh(labels, values, color='#3498db')

plt.gca().invert_yaxis()

plt.title('Топ-15 профессиональных IT-терминов ("Сущ" + "Сущ")', fontproperties = prop, fontsize=16)
plt.xlabel('Частота в корпусе', fontproperties = prop, fontsize=12)

plt.yticks(fontproperties=prop, fontsize=12)

plt.show()

top_vn = vn_freq.most_common(20)

labels = [item[0] for item in top_vn]
values = [item[1] for item in top_vn]

plt.figure(figsize=(10, 8))

plt.barh(labels, values, color='#3498db')

plt.gca().invert_yaxis()

plt.title('Топ-15 профессиональных IT-терминов ("Гл" + "Сущ")', fontproperties=prop, fontsize=16)
plt.xlabel('Частота в корпусе', fontproperties=prop, fontsize=12)

plt.yticks(fontproperties=prop, fontsize=12)

plt.show()