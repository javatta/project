# Sinotech Professionalism Checker
### Исследование корпуса китайской IT-документации: возможности анализа и автоматизации

Данный проект — исследовательский прототип, созданный в рамках программы ДПО «Компьютерная лингвистика». Работа посвящена анализу профессионального технического стиля китайского языка и оценке качества современных инструментов обработки естественного языка (NLP) при работе с узкоспециализированными текстами.

*Примечание: проект находится в стадии активной разработки. Файл `sinotech_tokenize_checker.py` является временным и будет удален в ближайшее время. Как только я пойму, как.*

## Цель проекта
Основная задача — доказать, что стандартные лингвистические модели (на примере spaCy) демонстрируют низкую точность при обработке китайской IT-терминологии.

## Аналитические возможности и результаты
В ходе исследования реализован полный цикл обработки данных, результаты которого представлены в виде визуализаций:

1. **Анализ лексической плотности (TTR).** Математически подтверждено, что профессиональная документация (API Docs) обладает на 20-40% более низкой лексической вариативностью по сравнению с README. Это свидетельствует о высокой степени стандартизации и повторяемости терминологии в профессиональной среде.
2. **Выявление терминологического ядра.** С помощью N-грамм выделен «скелет» китайской IT-лексики — устойчивые пары, которые часто отсутствуют в словарях (например, БКРС) или представлены в них без учета технического контекста.

## Оценка качества токенизации
Ключевой вывод проекта: стандартные токенизаторы (spaCy) часто некорректно сегментируют IT-термины, разбивая их на бессмысленные одиночные иероглифы (например, 预训练 превращается в 预训 + 练). Проект демонстрирует методику «донастройки» сегментации через внедрение специализированных словарей, извлеченных из живых корпусов (Baidu, HanLP).

## Модуль диагностики (Smart Checker): вскрытие проблем анализа
В проект включен экспериментальный модуль статистической проверки. В текущей итерации он выполняет роль диагностического стенда, который наглядно демонстрирует две фундаментальные проблемы при автоматической обработке китайского ИТ-языка:
* **Артефакты сегментации.** Модуль часто предлагает некорректные варианты (например, глагол 预训), что является прямым следствием ошибки токенизатора, который не узнал термин 预训练 (предобучение) и разрезал его на части.
* **Смешение семантических ролей.** Из-за отсутствия морфологии в китайском языке система часто предлагает слова, которые в ИТ-контексте могут быть и существительными, и глаголами (например, 输入 - ввод/вводить, 定义 - определение/определять).
* **Модуль** наглядно показывает, что простого статистического сопоставления слов (биграмм) и классической pos токенизации недостаточно для создания качественного ИИ-ассистента. Без внедрения специализированных словарей и учета контекста система выдает «сырые» данные, требующие экспертной фильтрации.

## Технологический стек
* **Сбор данных:** GitHub REST API (парсинг документации PaddleNLP, HanLP, профессиональной литературы).
* **Обработка:** spaCy (модель zh_core_web_sm), NLTK.
* **Визуализация и метрики:** Pandas, Matplotlib (с интеграцией китайских шрифтов).

## Инструкция по запуску
1. Откройте файл `Sinotech_Professionalism_Checker.ipynb` в среде Google Colab.
2. Запустите все ячейки по порядку (Runtime -> Run all).
3. Скрипт автоматически скачает необходимые корпуса, установит шрифты и сгенерирует аналитические графики.
4. В финальной части ноутбука можно протестировать работу Smart Checker на собственном фрагменте текста.
векторных представлений слов (Embeddings) для поиска контекстуальных синонимов и интеграция инструмента в системы автоматизированного перевода.

**Автор:** Бабак Виктория
